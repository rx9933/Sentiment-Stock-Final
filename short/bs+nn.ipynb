{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/mili/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Gradient dV_dS is None. Ensure the model output depends on S.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 176\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Run the main function with a list of tickers\u001b[39;00m\n\u001b[1;32m    175\u001b[0m tickers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTSLA\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Add more tickers as needed\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 163\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(tickers)\u001b[0m\n\u001b[1;32m    157\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    158\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[1;32m    159\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: black_scholes_pde_loss(y_true, y_pred, S_input, sigma, r, T)\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Predict and plot\u001b[39;00m\n\u001b[1;32m    171\u001b[0m predict_and_plot(ticker, processed_data, sequences, model, seq_len, pred_len, scaler, cut_off)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[3], line 159\u001b[0m, in \u001b[0;36mmain.<locals>.<lambda>\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Build and compile model\u001b[39;00m\n\u001b[1;32m    156\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(input_shape\u001b[38;5;241m=\u001b[39m(seq_len, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), output_len\u001b[38;5;241m=\u001b[39mpred_len)\n\u001b[1;32m    157\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    158\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[0;32m--> 159\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: \u001b[43mblack_scholes_pde_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    163\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    164\u001b[0m     X_train, y_train,\n\u001b[1;32m    165\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    166\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    167\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    168\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 71\u001b[0m, in \u001b[0;36mblack_scholes_pde_loss\u001b[0;34m(y_true, y_pred, S, sigma, r, T)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Ensure dV_dS is not None before computing the second derivative\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dV_dS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient dV_dS is None. Ensure the model output depends on S.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m d2V_dS2 \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(dV_dS, S)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Time derivative (assuming y_true is the target future price)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Gradient dV_dS is None. Ensure the model output depends on S."
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Fetch stock data from Yahoo Finance\n",
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Compute technical indicators\n",
    "def compute_technical_indicators(data):\n",
    "    data['SMA30'] = data['Close'].rolling(window=30).mean()\n",
    "    data['SMA100'] = data['Close'].rolling(window=100).mean()\n",
    "    vol_window = 21\n",
    "    data['Volatility'] = data['Close'].pct_change().rolling(window=vol_window).std() * np.sqrt(252)\n",
    "    return data.dropna()\n",
    "\n",
    "\n",
    "# Normalize and apply scaling\n",
    "def preprocess_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return data_scaled, scaler\n",
    "\n",
    "\n",
    "# Create sequences for time-series prediction\n",
    "def create_sequences(data, seq_len, pred_len):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_len - pred_len + 1):\n",
    "        sequences.append(data[i:i + seq_len + pred_len])\n",
    "    return np.array(sequences)\n",
    "\n",
    "\n",
    "# Build a neural network model\n",
    "def build_model(input_shape, output_len):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(180),\n",
    "        LeakyReLU(),\n",
    "        Dense(360),\n",
    "        LeakyReLU(),\n",
    "        Dense(360),\n",
    "        LeakyReLU(),\n",
    "        Dense(output_len)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the Black-Scholes PDE Loss Function\n",
    "def black_scholes_pde_loss(y_true, y_pred, S, sigma, r, T):\n",
    "    # Ensure `S` is part of the computation graph\n",
    "    S = tf.convert_to_tensor(S, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(S)\n",
    "        V = y_pred  # Assuming V is the model's output\n",
    "        dV_dS = tape.gradient(V, S)\n",
    "\n",
    "    # Ensure dV_dS is not None before computing the second derivative\n",
    "    if dV_dS is None:\n",
    "        raise ValueError(\"Gradient dV_dS is None. Ensure the model output depends on S.\")\n",
    "\n",
    "    d2V_dS2 = tape.gradient(dV_dS, S)\n",
    "\n",
    "    # Time derivative (assuming y_true is the target future price)\n",
    "    dV_dt = (y_pred - y_true) / T\n",
    "\n",
    "    # Black-Scholes PDE residual\n",
    "    pde_residual = (\n",
    "        dV_dt +\n",
    "        0.5 * sigma**2 * S**2 * d2V_dS2 +\n",
    "        r * S * dV_dS -\n",
    "        r * y_pred\n",
    "    )\n",
    "\n",
    "    mse_pde = tf.reduce_mean(tf.square(pde_residual))\n",
    "    return mse_pde\n",
    "\n",
    "\n",
    "# Predict and Plot\n",
    "def predict_and_plot(ticker, data, s, model, seq_len, pred_len, scaler, cut_off):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    red_x, red_y, black_y, blue_x, blue_y, green_y = [], [], [], [], [], []\n",
    "\n",
    "    for idx in range(0, s.shape[0], pred_len):\n",
    "        future_input = s[idx, :seq_len, :].reshape(1, seq_len, -1)\n",
    "        future = model.predict(future_input)[0]\n",
    "        future_padded = np.hstack((future.reshape(-1, 1), np.zeros((future.shape[0], data.shape[1] - 1))))\n",
    "        future_transform = scaler.inverse_transform(future_padded)[:, 0]\n",
    "\n",
    "        future_true = s[idx, seq_len:, 0].reshape(-1, 1)\n",
    "        future_true_padded = np.hstack((future_true, np.zeros((future_true.shape[0], data.shape[1] - 1))))\n",
    "        future_true_transform = scaler.inverse_transform(future_true_padded)[:, 0]\n",
    "\n",
    "        if idx < cut_off:\n",
    "            red_x.append(idx)\n",
    "            red_y.append(future_transform)\n",
    "            black_y.append(future_true_transform)\n",
    "        else:\n",
    "            blue_x.append(idx)\n",
    "            blue_y.append(future_transform)\n",
    "            green_y.append(future_true_transform)\n",
    "\n",
    "    plt.plot(red_x, red_y, color=\"red\", label=\"Train Prediction\")\n",
    "    plt.plot(red_x, black_y, color=\"black\", label=\"Train True\")\n",
    "    plt.plot(blue_x, blue_y, color=\"blue\", label=\"Test Prediction\")\n",
    "    plt.plot(blue_x, green_y, color=\"green\", label=\"Test True\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Predictions for {ticker}\")\n",
    "    plt.xlabel(\"Trading Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main workflow\n",
    "def main(tickers):\n",
    "    for ticker in tickers:\n",
    "        # Fetch data\n",
    "        stock_data = fetch_stock_data(ticker, start_date=\"2021-01-01\", end_date=\"2024-09-30\")\n",
    "        stock_data = compute_technical_indicators(stock_data)\n",
    "\n",
    "        # Select features\n",
    "        features = ['Close', 'Volume', 'SMA30', 'SMA100', 'Volatility']\n",
    "        data = stock_data[features].dropna().copy()\n",
    "        processed_data, scaler = preprocess_data(data)\n",
    "\n",
    "        # Create sequences\n",
    "        seq_len = 180  # 6 months of data\n",
    "        pred_len = 20  # Predict next 20 days\n",
    "        sequences = create_sequences(processed_data, seq_len, pred_len)\n",
    "\n",
    "        cut_off = int(0.8 * sequences.shape[0])\n",
    "        X_train = sequences[:cut_off, :seq_len, :]\n",
    "        X_test = sequences[cut_off:, :seq_len, :]\n",
    "        y_train = sequences[:cut_off, seq_len:, 0]\n",
    "        y_test = sequences[cut_off:, seq_len:, 0]\n",
    "\n",
    "        # Define asset price input for loss function\n",
    "        S_input = data['Close'].values.reshape(-1, 1)\n",
    "        sigma = 0.2  # Example volatility\n",
    "        r = 0.01  # Example risk-free rate\n",
    "        T = 30 / 252  # Example time to maturity\n",
    "\n",
    "        # Build and compile model\n",
    "        model = build_model(input_shape=(seq_len, X_train.shape[2]), output_len=pred_len)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss=lambda y_true, y_pred: black_scholes_pde_loss(y_true, y_pred, S_input, sigma, r, T)\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Predict and plot\n",
    "        predict_and_plot(ticker, processed_data, sequences, model, seq_len, pred_len, scaler, cut_off)\n",
    "\n",
    "    \n",
    "# Run the main function with a list of tickers\n",
    "tickers = ['TSLA']  # Add more tickers as needed\n",
    "main(tickers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
