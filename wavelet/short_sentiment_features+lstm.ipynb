{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 19:33:51.908369: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-03 19:33:59.631184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733276041.046626    1190 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733276041.900399    1190 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 19:34:04.910726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import norm\n",
    "from scipy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Fetch stock data from Yahoo Finance\n",
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "# Compute technical indicators\n",
    "def compute_technical_indicators(data):\n",
    "    data['SMA30'] = data['Close'].rolling(window=1).mean()\n",
    "    data['SMA100'] = data['Close'].rolling(window=1).mean()\n",
    "    vol_window = 1\n",
    "    data['Volatility'] = data['Close'].pct_change().rolling(window=vol_window).std() * np.sqrt(252)\n",
    "    return data.dropna()\n",
    "\n",
    "# Black-Scholes option pricing model\n",
    "def black_scholes(S, K, T, r, sigma, option_type='call'):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    if option_type == 'call':\n",
    "        return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "    elif option_type == 'put':\n",
    "        return K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "\n",
    "# Add option pricing features\n",
    "def add_option_pricing_features(data):\n",
    "    strike_price = data['Close'] * 1.05\n",
    "    time_to_maturity = 30 / 252\n",
    "    risk_free_rate = 0.01\n",
    "    option_type = 'call'\n",
    "    data['Option_Price'] = black_scholes(data['Close'], strike_price, time_to_maturity, risk_free_rate, data['Volatility'], option_type)\n",
    "    data['Implied_Volatility'] = data['Volatility']\n",
    "    return data\n",
    "\n",
    "# Normalize and preprocess data\n",
    "def preprocess_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    return data_scaled, scaler\n",
    "\n",
    "# Create sequences for time-series prediction\n",
    "def create_sequences(data, seq_len, pred_len):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_len - pred_len + 1):\n",
    "        sequences.append(data[i:i + seq_len + pred_len])\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Create a Hankel matrix\n",
    "def create_hankel_matrix(time_series, window_size):\n",
    "    return np.array([time_series[i:i + window_size] for i in range(len(time_series) - window_size + 1)])\n",
    "\n",
    "# Perform SVD on the Hankel matrix\n",
    "def hankel_svd(hankel_matrix):\n",
    "    U, Sigma, Vt = svd(hankel_matrix, full_matrices=False)\n",
    "    return U, Sigma, Vt\n",
    "\n",
    "# Build LSTM model\n",
    "def build_model(input_shape, output_len):\n",
    "    model = Sequential([\n",
    "        LSTM(180, activation='relu', return_sequences=False),\n",
    "        Dense(180), LeakyReLU(),\n",
    "        Dense(360), LeakyReLU(),\n",
    "        Dense(360), LeakyReLU(),\n",
    "        Dense(output_len)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Fetch news and perform sentiment analysis\n",
    "def fetch_news_and_sentiment(ticker, start_date, end_date):\n",
    "    headlines, dates = [], []\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    curr_page = 1\n",
    "\n",
    "    while len(headlines) < 1000:\n",
    "        api_url = f\"https://newsapi.org/v2/everything?q={ticker}&from={start_date}&to={end_date}&apiKey=7e9676fa277d423f8deff9a4894bd717&page={curr_page}&language=en\"\n",
    "        response = requests.get(api_url)\n",
    "        news_data = response.json()\n",
    "        print(news_data)\n",
    "        if \"articles\" in news_data:\n",
    "            for article in news_data['articles']:\n",
    "                headlines.append(article['title'])\n",
    "                dates.append(article['publishedAt'][:10])\n",
    "        curr_page += 1\n",
    "        if len(news_data['articles']) == 0: break\n",
    "\n",
    "    sentiment_data = []\n",
    "    for headline, date in zip(headlines, dates):\n",
    "        score = analyzer.polarity_scores(headline)\n",
    "        sentiment_data.append({'date': date, 'sentiment': score['compound']})\n",
    "    return sentiment_data\n",
    "\n",
    "# Predict and plot results\n",
    "\n",
    "def predict_and_plot(m, ticker, data, s, model, seq_len, pred_len, scaler, cut_off):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    rmse_values = []  # To store RMSE values for each section\n",
    "    section_start_idx = []  # To store the start indices of each 30-day section for placement of RMSE labels\n",
    "    #s.shape[0]\n",
    "    pred_count = 0\n",
    "    red_x,red_y, black_y, blue_x, blue_y, green_y = [],[],[],[],[],[]\n",
    "\n",
    "    for idx in range(0,s.shape[0], pred_len):  # Loop through each 30-day block\n",
    "        print(\"IDX\",idx)\n",
    "    # for idx in range(1): # only first 30 after being trained\n",
    "        # Input remains in 3D for predictio\n",
    "        future_input = s[idx, :seq_len, :].reshape(1, seq_len, -1)  # Keep the 3D shape\n",
    "        future = model.predict(future_input)[0]  # Predict future \"Close\" prices\n",
    "        # Pad the prediction with zeros for the \"Volume\" and \"SMA\" columns\n",
    "        future_padded = np.hstack((future.reshape(-1, 1), np.zeros((future.shape[0], data.shape[1] - 1))))  # Adjust for PCA features\n",
    "\n",
    "        # Inverse transform the padded prediction\n",
    "        future_transform = scaler.inverse_transform(future_padded)[:, 0]  # Extract \"Close\"\n",
    "\n",
    "        # Extract and inverse transform the true \"Close\" values\n",
    "        future_true = s[idx, seq_len:, 0].reshape(-1, 1)  # Use only \"Close\"\n",
    "        future_true_padded = np.hstack((future_true, np.zeros((future_true.shape[0], data.shape[1] - 1))))  # Adjust for PCA features\n",
    "        future_true_transform = scaler.inverse_transform(future_true_padded)[:, 0]  # Extract \"Close\"\n",
    "\n",
    "        # Calculate RMSE for this 30-day section\n",
    "        rmse = np.sqrt(mean_squared_error(future_true_transform, future_transform))\n",
    "        rmse_values.append(rmse)\n",
    "        section_start_idx.append(idx)  # Store the start index for the RMSE label\n",
    "        if idx< cut_off:\n",
    "            red_x +=[idx]\n",
    "            red_y+=[future_transform]\n",
    "            black_y+=[future_true_transform]\n",
    "        else:\n",
    "            blue_x +=[idx]\n",
    "            blue_y+=[future_transform]\n",
    "            green_y+=[future_true_transform]\n",
    "\n",
    "        # # Plot the predictions\n",
    "        # plt.scatter(idx, future_transform, \n",
    "        # # label=\"Train Prediction\" if idx < cut_off else \"Test Prediction\", \n",
    "        #     color=\"red\" if idx < cut_off else \"blue\")\n",
    "\n",
    "\n",
    "        # # plt.plot(np.arange(idx, idx + pred_len), future_transform, \n",
    "        # #         # label=\"Train Prediction\" if idx < cut_off else \"Test Prediction\", \n",
    "        # #          color=\"red\" if idx < cut_off else \"blue\")\n",
    "\n",
    "        # # Plot the true values\n",
    "        # plt.scatter(idx, future_true_transform, \n",
    "        #          #label=\"Train True\" if idx < cut_off else \"Test True\", \n",
    "        #          color=\"black\" if idx < cut_off else \"green\")\n",
    "\n",
    "        \n",
    "        if idx >= cut_off:\n",
    "            pred_count+=1\n",
    "    plt.plot(red_x, red_y, color = \"red\")\n",
    "    plt.plot(red_x, black_y, color = \"black\")\n",
    "    plt.plot(blue_x, blue_y, color = \"blue\")\n",
    "    plt.plot(blue_x, green_y, color = \"green\")\n",
    "    # Add legend and grid\n",
    "    # plt.legend([\"Train Prediction\", \"Train True\", \"Test Prediction\", \"Test True\"])\n",
    "    plt.plot([], [], color=\"red\", label=\"Train Prediction\")  # Dummy plot for legend\n",
    "    plt.plot([], [], color=\"black\", label=\"Train True\")      # Dummy plot for legend\n",
    "    plt.plot([], [], color=\"blue\", label=\"Test Prediction\")  # Dummy plot for legend\n",
    "    plt.plot([], [], color=\"green\", label=\"Test True\")      # Dummy plot for legend\n",
    "    plt.legend()\n",
    "    # for idx in range(0, s.shape[0], pred_len): \n",
    "    # Add vertical line to separate each 30-day section\n",
    "        # plt.axvline(x=idx + pred_len, color='gray', linestyle='--', linewidth=1)\n",
    "    # Title and labels\n",
    "    \n",
    "    plt.title(f\"{seq_len}-{pred_len} Predictions for {ticker}, model {m}\")\n",
    "    plt.xlabel(\"Trading Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "\n",
    "    # Add RMSE labels below the x-axis at the start of each 30-day section\n",
    "    # for i, rmse in zip(section_start_idx, rmse_values):\n",
    "    #     plt.text(i + pred_len / 2, min(plt.ylim()), f\"{rmse:.0f}\", color=\"teal\", fontsize=10, ha='center', va='top')\n",
    "    # plt.text(- 5- pred_len / 2, min(plt.ylim()), f\"RMSE\", color=\"teal\", fontsize=10, ha='center', va='top')\n",
    "    # print(rmse_values)\n",
    "    # plt.grid(True)\n",
    "\n",
    "    if m == \"LSTM w/ 2 features (close price, sentiment)\":\n",
    "        dir = f\"{seq_len}-{pred_len}/sentLSTM_2\"\n",
    "        if not os.path.exists(dir):\n",
    "                os.makedirs(dir)\n",
    "        plt.savefig(f\"{dir}/{ticker}.png\")\n",
    "    return np.mean(rmse_values[-pred_count]) # testing rmse\n",
    "\n",
    "\n",
    "# Main workflow\n",
    "# def main(tickers):\n",
    "#     m = \"sentLSTM w/ 1 feature\"\n",
    "#     test_rmse = []\n",
    "#     for ticker in tickers:\n",
    "#         tsla = fetch_stock_data(ticker, start_date=\"2024-11-03\", end_date=\"2024-12-01\")\n",
    "#         tsla = compute_technical_indicators(tsla)\n",
    "#         tsla = add_option_pricing_features(tsla)\n",
    "\n",
    "#         sentiment_data = fetch_news_and_sentiment(ticker, \"2024-11-03\", \"2024-12-01\")\n",
    "#         sentiment_df = pd.DataFrame(sentiment_data)\n",
    "#         sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "#         tsla.reset_index(inplace=True)  # Make the index a regular column\n",
    "#         tsla['Date'] = pd.to_datetime(tsla['Date'])  # Ensure Date is a proper datetime type\n",
    "#         sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])  # Ensure date compatibility\n",
    "\n",
    "#         # Merge on the Date and date columns\n",
    "#         tsla = pd.merge(tsla, sentiment_df, left_on='Date', right_on='date', how='left')\n",
    "#         tsla['sentiment'] = tsla['sentiment'].fillna(0)  # Fill missing sentiment with 0\n",
    "#         #'Volume', 'SMA30', 'SMA100', 'Volatility', \n",
    "\n",
    "#         features = ['Close', 'sentiment']\n",
    "#         data = tsla[features].copy()\n",
    "#         processed_data, scaler = preprocess_data(data)\n",
    "        \n",
    "def main(tickers):\n",
    "    m = \"LSTM w/ 2 features (close price, sentiment)\"\n",
    "    test_rmse = []\n",
    "    for ticker in tickers:\n",
    "        tsla = fetch_stock_data(ticker, start_date=\"2024-11-03\", end_date=\"2024-12-01\")\n",
    "        if tsla.empty:\n",
    "            print(f\"No data fetched for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        tsla = compute_technical_indicators(tsla)\n",
    "        if tsla.empty:\n",
    "            print(f\"Technical indicators computation resulted in empty data for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        tsla = add_option_pricing_features(tsla)\n",
    "        if tsla.empty:\n",
    "            print(f\"Option pricing features resulted in empty data for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        sentiment_data = fetch_news_and_sentiment(ticker, \"2024-11-03\", \"2024-12-01\")\n",
    "        sentiment_df = pd.DataFrame(sentiment_data)\n",
    "        # Group by date and average sentiment\n",
    "        sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])  # Ensure 'date' is in datetime format\n",
    "        sentiment_df = sentiment_df.groupby('date', as_index=False)['sentiment'].mean()\n",
    "\n",
    "        print(\"SLA,\", sentiment_df)\n",
    "        sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "        tsla.reset_index(inplace=True)\n",
    "        tsla['Date'] = pd.to_datetime(tsla['Date'])\n",
    "        sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "\n",
    "        tsla = pd.merge(tsla, sentiment_df, left_on='Date', right_on='date', how='left')\n",
    "        tsla['sentiment'] = tsla['sentiment'].fillna(0)\n",
    "        print(tsla.shape)\n",
    "        features = ['Close', 'sentiment']\n",
    "        # features = ['Close', 'Volume', 'SMA30', 'SMA100', 'Volatility', 'Option_Price']\n",
    "        data = tsla[features].copy()\n",
    "        if data.empty:\n",
    "            print(f\"No valid data after filtering for {ticker}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Data shape before preprocessing: {data.shape}\")\n",
    "        processed_data, scaler = preprocess_data(data)\n",
    "\n",
    "        window_size = 2\n",
    "        hankel_matrix = create_hankel_matrix(data['Close'].values, window_size)\n",
    "        U, Sigma, Vt = hankel_svd(hankel_matrix)\n",
    "        dmd_modes = U[:, :2]\n",
    "        processed_data_trimmed = processed_data[-dmd_modes.shape[0]:]\n",
    "        data_combined = np.hstack((processed_data_trimmed, dmd_modes))\n",
    "\n",
    "        data_combined = processed_data_trimmed\n",
    "\n",
    "        seq_len, pred_len = 1, 1\n",
    "        sequences = create_sequences(data_combined, seq_len, pred_len)\n",
    "\n",
    "        cut_off = int(0.8 * sequences.shape[0])\n",
    "        print(\"cutoff\",cut_off)\n",
    "        print(\"Data.shape\", data_combined.shape)\n",
    "\n",
    "        X_train = sequences[:cut_off, :seq_len, :]\n",
    "        X_test = sequences[cut_off:, :seq_len, :]\n",
    "        y_train = sequences[:cut_off, seq_len:, 0]\n",
    "        y_test = sequences[cut_off:, seq_len:, 0]\n",
    "\n",
    "        model = build_model(input_shape=(seq_len, X_train.shape[2]), output_len=pred_len)\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "        test_rmse += [predict_and_plot(m, ticker, data_combined, sequences, model, seq_len, pred_len, scaler, cut_off)]\n",
    "    print(f\"Average Test RMSE: {np.mean(test_rmse)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical indicators computation resulted in empty data for TSLA\n",
      "Average Test RMSE: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/mili/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/mili/.local/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "main([\"TSLA\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
